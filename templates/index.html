<!DOCTYPE html>
<!--[if lt IE 7]>      <html lang="en" class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html lang="en" class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html lang="en" class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html lang="en" class="no-js"> <!--<![endif]-->
    <head>
    	<!-- meta charec set -->
        <meta charset="utf-8">
    <!-- Always force latest IE rendering engine or request Chrome Frame -->
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <!-- Page Title -->
        <title>Emotion Detection</title>   
        <link rel="icon" type="image/png" href="{{url_for('static', filename = "assetsmain/favicon.ico")}}">
    <!-- Mobile Specific Meta -->
        <meta name="viewport" content="width=device-width, initial-scale=1">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script src="{{url_for('static', filename = "assets/js/jquery.redirect.js")}}"></script>
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap-theme.min.css">
        <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"> </script>

  
    
    <script src="https://download.affectiva.com/js/3.2/affdex.js"> </script>
    
<style type="text/css" media="screen">
      
      
      
      
    </style>
    

    </head>
	
    <body>
  <div class="container-fluid">
    <div class="row">
      <br/>
      <div class="col-md-1"></div>
    <div class="col-md-8" id="affdex_elements" style="width:672px;height:486px;border-style: solid;"></div>
      <div class="col-md-4">
        <div style="height:25em;">
          <strong><h3><p style="font-family: 'Comic Sans MS', cursive, sans-serif;">Emotion Tracking Results</p></h3></strong>
          <h5><p style="font-family: 'Comic Sans MS', cursive, sans-serif;"><div id="results" style="word-wrap:break-word;"></div></p></h5>
        </div>
        <div>
          <strong><h3><p style="font-family: 'Comic Sans MS', cursive, sans-serif;">Detector Log Messages</p></h3></strong>
        </div>
        <h5><p style="font-family: 'Comic Sans MS', cursive, sans-serif;"><div id="logs"></div></p></h5>
      </div>
    </div>
    <div>
      <br/>
      <button id="start" class="btn btn-info" onclick="onStart()">Start</button>
      <button id="stop" class="btn btn-danger" onclick="onStop()">Stop</button>
      <button id="reset" class="btn btn-warning" onclick="onReset()">Reset</button>
    </div>
  </div>
 <script>
		      // SDK Needs to create video and canvas nodes in the DOM in order to function
      // Here we are adding those nodes a predefined div.
	  
      var divRoot = $("#affdex_elements")[0];
      var width = 640;
      var height = 480;
      var faceMode = affdex.FaceDetectorMode.LARGE_FACES;
      //Construct a CameraDetector and specify the image width / height and face detector mode.
      var detector = new affdex.CameraDetector(divRoot, width, height,faceMode);

      //Enable detection of all Expressions, Emotions and Emojis classifiers.
      detector.detectAllEmotions();
      detector.detectAllExpressions();
      detector.detectAllEmojis();
      detector.detectAllAppearance();

      //Add a callback to notify when the detector is initialized and ready for runing.
      detector.addEventListener("onInitializeSuccess", function() {
        log('#logs', "The detector reports initialized");
        //Display canvas instead of video feed because we want to draw the feature points on it
        $("#face_video_canvas").css("display", "block");
        $("#face_video").css("display", "none");
      });

      function log(node_name, msg) {
        $(node_name).append("<span>" + msg + "</span><br />")
      }

      //function executes when Start button is pushed.
      function onStart() {
        if (detector && !detector.isRunning) {
          //("#logs").html("");
          detector.start();
        }
        log('#logs', "Clicked the start button");
      }

      //function executes when the Stop button is pushed.
      function onStop() {
        log('#logs', "Clicked the stop button");
        if (detector && detector.isRunning) {
          detector.removeEventListener();
          detector.stop();
        }
      };

      //function executes when the Reset button is pushed.
      function onReset() {
        log('#logs', "Clicked the reset button");
        if (detector && detector.isRunning) {
          detector.reset();

          ('#results').html("");
        }
      };

      //Add a callback to notify when camera access is allowed
      detector.addEventListener("onWebcamConnectSuccess", function() {
        log('#logs', "Webcam access allowed");
      });

      //Add a callback to notify when camera access is denied
      detector.addEventListener("onWebcamConnectFailure", function() {
        log('#logs', "webcam denied");
        console.log("Webcam access denied");
      });

      //Add a callback to notify when detector is stopped
      detector.addEventListener("onStopSuccess", function() {
        log('#logs', "The detector reports stopped");
        ("#results").html("");
      });

      //Add a callback to receive the results from processing an image.
      //The faces object contains the list of the faces detected in an image.
      //Faces object contains probabilities for all the different expressions, emotions and appearance metrics
      detector.addEventListener("onImageResultsSuccess", function(faces, image, timestamp) {
        $('#results').html("");
        log('#results', "Timestamp: " + timestamp.toFixed(2));
        log('#results', "Number of faces found: " + faces.length);
        if (faces.length > 0) {
          log('#results', "Appearance: " + JSON.stringify(faces[0].appearance));
          log('#results', "Emotions: " + JSON.stringify(faces[0].emotions, 

function(key, val) {
            return val.toFixed ? Number(val.toFixed(0)) : val;
          }));
          log('#results', "Expressions: " + JSON.stringify(faces[0].expressions, function(key, val) {
            return val.toFixed ? Number(val.toFixed(0)) : val;
          }));
          log('#results', "Emoji: " + faces[0].emojis.dominantEmoji);
          drawFeaturePoints(image, faces[0].featurePoints);
          var kk=JSON.stringify(faces)
          if(timestamp.toFixed(2)>25.00&&timestamp.toFixed(2)<25.25)
          {
            $.redirect("/faceData", {'ok':faces[0].emotions});
          }
      //$.redirect("/faceData", JSON.stringify(faces[0], null, '\t') );
    
          //$.post("/ok",{"data":kk});
        }
      });

      //Draw the detected facial feature points on the image
      function drawFeaturePoints(img, featurePoints) {
        var contxt = $('#face_video_canvas')[0].getContext('2d');

        var hRatio = contxt.canvas.width / img.width;
        var vRatio = contxt.canvas.height / img.height;
        var ratio = Math.min(hRatio, vRatio);

        contxt.strokeStyle = "#FFFFFF";
        for (var id in featurePoints) {
          contxt.beginPath();
          contxt.arc(featurePoints[id].x, featurePoints[id].y, 2, 0, 2 * Math.PI);
          contxt.stroke();

        }
      }
</script>
<script type="text/javascript" src="https://chat.botplatform.io/plugin-v2.js?bot=bot_1522412784528"></script>
<script type="text/javascript">
    window.botplatform.init({});
</script>
<script type="text/javascript">
        setTimeout(function(){
          url_string = window.location.href
url = new URL(url_string);
console.log("SSDD")
console.log(url)
    iframe = document.getElementById('ymIframeId');
    iframe.contentWindow.postMessage(JSON.stringify({event_code: 'ym-client-event', data: JSON.stringify({
      event: {
        code: "Firsts",
        data:{
          description:url.searchParams.get("name"),
          },
          
      }
    })}), 'https://chat.botplatform.io');
  },3000)
    </script>
</body>

</html>
